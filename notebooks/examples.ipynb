{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3481ed3d",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Here we provide some examples with real-world data to examplify the difference between the different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450d41d",
   "metadata": {},
   "source": [
    "## Breast Cancer Prediction\n",
    "In this example, we look at the Breast Cancer dataset from scikit-learn. The classification task is to predict whether the subject has cancer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2ee6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import our custom functions\n",
    "from src.metrics import create_metrics\n",
    "from src.cv import run_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072ae8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (569, 30)\n",
      "Class distribution:\n",
      "357 positive, 212 negative, ratio: 0.63\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# print dataset shape\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "# Check class balance\n",
    "print(f\"Class distribution:\\n{sum(y)} positive, {len(y) - sum(y)} negative, ratio: {sum(y) / len(y):.2f}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd5de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           average    pooled\n",
      "accuracy  0.978910  0.978910\n",
      "auc       0.995428  0.995098\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "metrics = create_metrics([\"accuracy\", \"auc\"])\n",
    "results = run_cv(model, X_scaled, y, metrics, n_splits=5, stratified=True, random_state=1)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac9780",
   "metadata": {},
   "source": [
    "## Cognitive Impairment\n",
    "In this example, we look at the Oasis3 data. We have cortical volume for various subjects and sessions that are cognitively normal or impaired. The classification task is to predict whether the subject is cogntiviely normal or not, based on cortical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64acfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2832, 104)\n",
      "Shape of baseline data: (1029, 104)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = Path(\"../data/oasis3_fs_mci.tsv\")\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# drop rows that have empty cells / NAs\n",
    "df = df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "# only keep first occurence of each subject\n",
    "df_baseline = df.drop_duplicates(subset=[\"subject\"], keep=\"first\")\n",
    "print(f\"Shape of baseline data: {df_baseline.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6446444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "755 positive, 274 negative, ratio: 0.73\n"
     ]
    }
   ],
   "source": [
    "# split into X\n",
    "X = df_baseline.drop(columns=[\"subject\", \"session\", \"age\", \"cognitiveyly_normal\"])\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "X = X.to_numpy()\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# and y\n",
    "y = df_baseline[\"cognitiveyly_normal\"].to_numpy()\n",
    "y_binary = (y == True).astype(int)\n",
    "\n",
    "# Check class balance\n",
    "print(f\"Class distribution:\\n{sum(y_binary)} positive, {len(y_binary) - sum(y_binary)} negative, ratio: {sum(y_binary) / len(y_binary):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291adee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           average    pooled\n",
      "accuracy  0.820214  0.820214\n",
      "auc       0.822567  0.822845\n"
     ]
    }
   ],
   "source": [
    "## Run cross-validation\n",
    "model = LogisticRegression(max_iter=1000000)\n",
    "metrics = create_metrics([\"accuracy\", \"rocauc\"])\n",
    "results = run_cv(model, X_scaled, y_binary, metrics, n_splits=5, stratified=True, random_state=1)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unbalancedcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
