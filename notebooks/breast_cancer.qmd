---
title: "Investigating Cross-Validation AUC Metrics with Real Datasets"
format:
  html:
    code-fold: true
    toc: true
    fig-width: 10
    fig-height: 6
jupyter: python3
---

## Introduction

In this document, we extend our investigation of cross-validation ROC-AUC metrics from simulated data to real-world datasets. We focus on comparing two approaches to calculating AUC in cross-validation:

1. **Pooled AUC**: Concatenate predictions from all folds, then calculate one AUC
2. **Average AUC**: Calculate AUC for each fold, then average the results

We'll use datasets from scikit-learn to examine how these metrics behave with real data and under different class balance conditions.

## Setup
```{python}
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Import our custom functions
from src.metrics import create_metrics
from src.cv import run_cv
```

```{python}
X, y = load_breast_cancer(return_X_y=True)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Run cross-validation with original balanced dataset
model = LogisticRegression()
metrics = create_metrics(["accuracy", "auc"])
results = run_cv(model, X_scaled, y, metrics, n_splits=5, stratified=True, random_state=1)

results_df = pd.DataFrame(results)
print(results_df)
```