---
title: "Simulating Unbalaced Cross Validation"
format:
  html:
    code-fold: true
jupyter: python3
---
### Overview

Estimating the accuarcy, or other metrics, for classification tasks in cross-validation settings is a standard thing to do. However, there are multiple ways to do it, and especially in in unbalanced settings, it is probably not as straight forward. This documents first introduces the generalization error and how to compute it using cross-validation. Further, we will look at the unbalanced case and run simulations. 

### Theoretical Generalization Error

The generalization error of a learned model $f_\theta$ is the expected loss $\ell(\cdot, \cdot)$ over the entire, unseen, data distribution $P(x, y)$:

\begin{equation}
\mathcal{E}_{\text{gen}} = \mathbb{E}_{(x, y) \sim P(x, y)} \left[ \ell(f_\theta(x), y) \right]
\end{equation}

However, $P(x, y)$ is obviously unknown, so we approximate the generalization error on hold-out data. If we have a large dataset, a simple hold-out set is sufficient. However, often we need to use approaches like Cross-Validation to get more robust estimates of the generalization error. Nevertheles, the underlying assumption is that the test data is drawn from our data generating distribution $P(x, y)$. We will revisit this assumption later. 

### K-Fold Cross-Validation

The dataset is partitioned into $K$ disjoint subsets (folds). For each fold $k \in \{1, \ldots, K\}$:

We define:

- $\mathcal{D}_{\text{train}}^{(k)} = (x_i,y_i)_{i=1}^{n_{\text{train}}^{(k)}}$: the training set of size $n_{\text{train}}^{(k)}$ used to fit the model.

- $\mathcal{D}_{\text{val}}^{(k)} = (x_j, y_j)_{j=1}^{n_{\text{val}}^{(k)}}$: the validation set of size $n_{\text{val}}^{(k)}$ used to evaluate model performance.

The empirical generalization error estimate is then computed as:

\begin{align}

\hat{\mathcal{E}}_{\text{gen}} &= \sum_{k=1}^{K}\frac{n_{\text{val}}^{(k)}}{N} E_{val}^{k}  \\
&= \sum_{k=1}^{K}\frac{n_{\text{val}}^{(k)}}{N}  \frac{1}{n_{\text{val}}^{(k)}} \sum_{(x_i, y_i) \in \mathcal{D}^{(k)}_{\text{val}}}^{n_{\text{val}}^{(k)}}  \ell(f^{(k)}(x_i), y_i) \\
&= \frac{1}{N} \sum_{k=1}^{K} \sum_{(x_i, y_i) \in \mathcal{D}^{(k)}_{\text{val}}}^{n_{\text{val}}^{(k)}} \ell(f^{(k)}(x_i), y_i) \\
&= \frac{1}{N} \sum_{(x_i, y_i) \in \mathcal{D}_{val}}^N \ell(f^{(k)}(x_i), y_i)
\end{align}

with $\mathcal{D}_{\text{val}} \in \{ \mathcal{D}_{\text{val}}^{(1)}, ..., \mathcal{D}_{\text{val}}^{(K)}  \}$. 
This shows that averaging the loss across tests sets and computing one overall loss is equivalent. 

### The unbalanced case
Now in the unbalanced case, the problem is not the question on how to compute the loss, but how the distribution is within the folds. The empircal generalization error assumes that $\mathcal{D}_{\text{val}}^{(k)} \sim P(x,y)$, which is likely not true in the unbalanced case. A different stregy, such as \textit{stratified} K-Fold coss validation would be more appropriate, as this ensures that the distribution in each fold matches the distribution in the entire dataset.  

## Simulations
To show this, we can run some simulations. 
```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn import metrics

# Set seed for reproducibility
np.random.seed(42)
```
First, we generate two classes and define the imbalance. Here, we set 20\% of the samples to be positive and 80\% to be negative. We draw 1000 samples from a binomial distribution: $y \sim \text{Binomial}(n=1, p=0.2)$, where $y = 1$ represents the positive class and $y = 0$ the negative class


```{python}
# Parameters
n_samples = 1000
pos_ratio = 0.1  # class imbalance: 20% positives

# Sample class labels: y ~ Bernoulli(pos_ratio)
y = np.random.binomial(1, pos_ratio, size=n_samples)
```
We further draw 1000 samples from normal distributions, where 
$x_1 \sim \mathcal{N}(\mu_1, 1)$ with $\mu_1 = -1$, and 
$x_2 \sim \mathcal{N}(\mu_2, 1)$ with $\mu_2 = 1$, both having unit variance.

```{python}
# Define class-conditional distributions
mu_0, sigma_0 = -1, 1
mu_1, sigma_1 = 1, 1

# Sample x ~ P(x | y)
x = np.where(
    y == 0,
    np.random.normal(mu_0, sigma_0, size=n_samples),
    np.random.normal(mu_1, sigma_1, size=n_samples)
)

# Reshape x for sklearn
X = x.reshape(-1, 1)

# visualize data
plt.hist(x[y == 0], bins=30, alpha=0.6, label="y = 0")
plt.hist(x[y == 1], bins=30, alpha=0.6, label="y = 1")
plt.xlabel("x")
plt.ylabel("Count")
plt.title("Class-conditional distributions of x")
plt.legend()
plt.show()
```

We use a simple logistic regression model with **accuracy** as the evaluation metric:

$$
\text{Accuracy} = \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left[ \hat{y}_i = y_i \right]
$$

This metric is used both when computing the final loss on the entire hold-out sets and when averaging across folds in the 5-fold cross-validation procedure.
```{python}
# %% function to compute the accuracy
def calculate_accuracy(predictions, targets):
    """
    Calculates the accuracy of the model's predictions.

    Args:
        predictions (torch.Tensor): Model predictions (e.g., logits or probabilities)
        targets (torch.Tensor): Ground truth labels

    Returns:
        float: Accuracy value between 0 and 1
    """
    # Check where predictions match targets
    correct_predictions = sum(predictions == targets)

    # Calculate accuracy
    accuracy = correct_predictions / len(targets)

    return accuracy
```

```{python}
clf = LogisticRegression(solver="lbfgs")

cv = KFold(n_splits=5)

# Iterate through cv and save predictions
predictions = []
predictions_proba = []
true_labels = []
cv_error = []
cv_auc = []

fold=0
for train_idx, test_idx in cv.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Count number of class 1 in training and test sets
    num_class_1_train = np.sum(y_train == 1)
    num_class_1_test = np.sum(y_test == 1)
    print(f"Fold {fold}: Class 1 in train = {num_class_1_train} / {len(y_train)}, Class 1 in test = {num_class_1_test} / {len(y_test)}")
    fold+=1

    # fit the model
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_pred_proba = clf.predict_proba(X_test)[:,1]
    predictions.extend(y_pred)
    predictions_proba.extend(y_pred_proba)
    true_labels.extend(y_test)

    # compute the loss per cv
    fold_error = calculate_accuracy(y_pred, y_test)
    cv_error.append(fold_error)

    # auc
    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba)
    auc = metrics.auc(fpr, tpr)
    cv_auc.append(auc)

global_accuracy = calculate_accuracy(np.array(predictions), np.array(true_labels))
print(f"Global Accuracy: {global_accuracy:.5f}")

# Compute local loss (mean of cv_error)
local_accuracy = np.mean(cv_error)
print(f"Mean Accuracy across folds: {local_accuracy:.5f}")

# compute AUC
clobal_fpr, global_tpr, thresholds = metrics.roc_curve(np.array(true_labels), np.array(predictions_proba))
global_auc = metrics.auc(clobal_fpr, global_tpr)
print(f"Global AUC: {global_auc:.5f}")

local_auc = np.mean(cv_auc)
print(f"Mean AUC across folds: {local_auc:.5f}")



```
As we can see, both results are exactly the same, so either way to compute the accuracy yields the same result.

Since we have full control over the data generation and we know $P(x,y)$, we can also generate new data and get a "true" estimate of the generaration error.
```{python}
y_val = np.random.binomial(1, pos_ratio, size=n_samples)
x_val = np.where(
    y_val == 0,
    np.random.normal(mu_0, sigma_0, size=n_samples),
    np.random.normal(mu_1, sigma_1, size=n_samples)
)
x_val = x_val.reshape(-1, 1)

y_pred_val = clf.predict(x_val)
auc_val = calculate_accuracy(np.array(y_pred_val), np.array(y_val))
print(f"Simulated accuracy: {auc_val:.5f}")
```
