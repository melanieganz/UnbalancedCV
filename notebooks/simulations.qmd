---
title: "Simulations of Cross‑Validation and ROC‑AUC under Class Imbalance"
format:
  html:
    code-fold: true
jupyter: python3
---

## Setup

```{python}
from src.experiments import experiment_simulation
```

# 1. Simple case
We simulate a two class problem $y\in{0,1}$. Both classes are drawn from a Guassian with $\mu_0=-1$ and $\mu_2=1$. We compute the AUC of the ROC and the Accuracy, both pooled or averaged across folds. 

```{python}
import pandas as pd
import matplotlib.pyplot as plt

results = []
for seed in range(1, 1001):
  result = experiment_simulation(pos_ratio=0.1, n_samples=200,mu0=-1, sigma0=1,mu1=0,sigma1=1, seed=seed, verbose=False)
  # Flatten the nested dictionary structure
  for key, metrics in result.items():
      flattened_result = {'type': key, **metrics}
      results.append(flattened_result)

# Convert the flattened results into a DataFrame
results_df = pd.DataFrame(results)

# Visualization
results_df.boxplot(column=['auc'], by='type', figsize=(10, 6))
plt.title("Distribution of Results by Type")
plt.suptitle("")  # Remove the default title
plt.ylabel("Values")
plt.show()

# Print the mean by type
mean_results = results_df.groupby('type')[['accuracy', 'auc']].mean()
print(mean_results)
```

```{python}
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Perform a t-test to compare AUC between pooled and average
# Filter the data for the two types
pooled_auc = results_df[results_df['type'] == 'pooled']['auc']
average_auc = results_df[results_df['type'] == 'average']['auc']

# Combine the data into a single DataFrame for analysis
comparison_df = results_df[results_df['type'].isin(['pooled', 'average'])]

# Perform an ANOVA test using statsmodels
model = ols('auc ~ C(type)', data=comparison_df).fit()

# Print the ANOVA table
print(model.summary())
```

