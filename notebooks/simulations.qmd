---
title: "Simulations of Cross‑Validation and ROC‑AUC under Class Imbalance"
format:
  html:
    code-fold: true
jupyter: python3
---

## Setup

```{python}
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

from src.experiments import experiment_simulation
from src.cv import run_cv
from src import simulations
from src.metrics import create_metrics
```

# Simple Case
## Simulate Data 
We simulate a two class problem $y\in{0,1}$. Both classes are drawn from a Guassian with $\mu_0=-1$ and $\mu_2=1$. We compute the AUC of the ROC and the Accuracy, both pooled or averaged across folds. 

```{python}
n_samples = 500
pos_ratio = 0.1
mu0 = -1
sigma0 = 1
mu1 = 1
sigma1 = 1
```

Let's look at some simulated data.
```{python}
# simulate
X, y = simulations.simulate_dataset(n_samples, pos_ratio, mu0, sigma0, mu1, sigma1, seed=1)

# plot the dataset
fig, ax = plt.subplots()
ax.set_title(f"Simulated dataset (pos_ratio={pos_ratio}, n_samples={n_samples})")
ax.set_xlabel("Feature value")
ax.set_ylabel("Count")
ax.hist(X[y == 0], bins=30, alpha=0.5, label="Class 0", color="blue")
ax.hist(X[y == 1], bins=30, alpha=0.5, label="Class 1", color="orange")
ax.legend()
plt.show()
```
## Cross-Validation
Now let's actually do some cross-validation. We will use a simple logistic regression to begin with.

```{python}
X, y = simulations.simulate_dataset(n_samples, pos_ratio, mu0, sigma0, mu1, sigma1, seed=1)

model = LogisticRegression()
metrics = create_metrics(["accuracy", "auc"])
results = run_cv(model, X, y, metrics, n_splits=5, stratified=True, random_state=1)
```

## Generalization Error
Since we simulate data, we can simulate a "true" test set to test the externalization set.
```{python}
X_gen, y_gen = simulations.simulate_dataset(1000, pos_ratio, mu0, sigma0, mu1, sigma1, seed=100)

# fit the model on the full training dataset
model.fit(X, y)

# Predict and score on fresh ("generalization") data
y_pred_gen = model.predict(X_gen)
y_proba_gen = model.predict_proba(X_gen)[:, 1]

# add generalization to the results
results["generalized"] = {}
for name, function in metrics.items():
    score = function(y_gen, y_pred_gen, y_proba_gen)
    results["generalized"][name] = score
```

## Results
Let's look at the results:
```{python}
results_df = pd.DataFrame(results)
print(results_df)
```

# Repeated Tests
Now let's repreat this experiment 1000 for some formal statistical testing.

```{python}
results = []
for seed in range(1, 1001):
    result = experiment_simulation(
        pos_ratio=pos_ratio,
        n_samples=n_samples,
        mu0=mu0,
        sigma0=sigma0,
        mu1=mu1,
        sigma1=sigma1,
        seed=seed,
        verbose=False,
    )
    # Flatten the nested dictionary structure
    for key, metrics in result.items():
        flattened_result = {"type": key, **metrics}
        results.append(flattened_result)

# Convert the flattened results into a DataFrame
results_df = pd.DataFrame(results)

# Visualization
results_df.boxplot(column=["auc"], by="type", figsize=(10, 6))
plt.title("Distribution of Results by Type")
plt.suptitle("")  # Remove the default title
plt.ylabel("Values")
plt.show()

# Print the mean by type
mean_results = results_df.groupby("type")[["accuracy", "auc"]].mean()
print(mean_results)
```

```{python}
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Perform a t-test to compare AUC between pooled and average
# Filter the data for the two types
pooled_auc = results_df[results_df["type"] == "pooled"]["auc"]
average_auc = results_df[results_df["type"] == "average"]["auc"]

# Combine the data into a single DataFrame for analysis
comparison_df = results_df[results_df["type"].isin(["pooled", "average"])]

# Perform an ANOVA test using statsmodels
model = ols("auc ~ C(type)", data=comparison_df).fit()

# Print the ANOVA table
print(model.summary())
```

